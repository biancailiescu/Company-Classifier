# -*- coding: utf-8 -*-
"""insurance_taxonomy_categories.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/169FVCuuOFnW5-gBs-HBVyvBojh3MHjfr
"""

import pandas as pd
import spacy
from sentence_transformers import SentenceTransformer
from umap import UMAP
from sklearn.cluster import HDBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score

file_path = '/content/drive/MyDrive/Colab Notebooks/Veridion challenge/insurance_taxonomy - insurance_taxonomy.csv'
df = pd.read_csv(file_path)

nlp = spacy.load("en_core_web_sm")
def lemmatize(text):
    if not isinstance(text, str) or text.strip() == "":
        return ""

    doc = nlp(text)
    filtered_tokens = []

    for token in doc:
        if token.pos_ in ["NOUN", "ADJ", "PROPN"] and not token.is_stop:
            filtered_tokens.append(token.lemma_.lower())

    return " ".join(filtered_tokens) if filtered_tokens else text.lower()

df['lemmatized_name'] = df['label'].apply(lemmatize)

model = SentenceTransformer('all-mpnet-base-v2')

embeddings = model.encode(df['lemmatized_name'].tolist(), show_progress_bar=True)

umap_emb = UMAP(n_components=5, random_state=42).fit_transform(embeddings)

from sklearn.cluster import KMeans
num_clusters = 40
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
clusters = kmeans.fit_predict(embeddings)

df['cluster'] = clusters

clusters = df.groupby('cluster')

for cluster_id, cluster_data in clusters:
    print(f"Cluster {cluster_id}:")
    print(cluster_data[['label']])
    print("\n")

condition = df["label"].str.contains("Software", na=False)

df.loc[condition, "cluster"] = 40

print(df["cluster"].value_counts())

silhouette_avg = silhouette_score(embeddings, df['cluster'])
davies_bouldin_avg = davies_bouldin_score(embeddings, df['cluster'])

print(f"Silhouette Score: {silhouette_avg:.2f}")
print(f"Davies-Bouldin Score: {davies_bouldin_avg:.2f}")

insurance_category = {
    0: "Painting Services",
    1: "Cleaning Operations",
    2: "Manufacturing Services",
    3: "Training Services",
    4: "Construction Services",
    5: "House Services",
    6: "Animal Food Processing",
    7: "Signage Installation",
    8: "Gas Installation",
    9: "Furniture Manufacturing",
    10: "Communication Equipment",
    11: "Consulting Services",
    12: "Veterinary Services",
    13: "Plumbing Services",
    14: "Snow Removal Services",
    15: "Seeds",
    16: "Property Management Services",
    17: "Window Manufacturing",
    18: "Social Media Services",
    19: "Accessory Manufacturing",
    20: "Volunteering Services",
    21: "Data Analysis Services",
    22: "Excavation",
    23: "Media Production Services",
    24: "Agricultural Services",
    25: "Road Construction",
    26: "Marketing Services",
    27: "Boiler Installation",
    28: "Plastic Manufacturing",
    29: "Foundation Construction",
    30: "Apparel Manufacturing",
    31: "Drainage",
    32: "Food Processing",
    33: "Fire Protection",
    34: "Tree Services",
    35: "Soil",
    36: "Glass Installation",
    37: "Paper Services",
    38: "Electrical Services",
    39: "Health Services",
    40: "Software Development"
    }


df['insurance_category'] = df['cluster'].map(insurance_category)

df.drop(columns=['lemmatized_name'], inplace=True)

print(df['insurance_category'])

output_file_path = '/content/drive/MyDrive/Colab Notebooks/Veridion challenge/clusterized_labels.csv'
df.to_csv(output_file_path, index=False)
print(f"File saved to {output_file_path}")