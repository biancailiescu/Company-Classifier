# -*- coding: utf-8 -*-
"""companies_with_categories.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zfGv-Xn7DrNY-VKwCz4Vi2QtdensFQPK
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
import torch
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import StandardScaler

nltk.download('stopwords')
nltk.download('punkt_tab')
nltk.download('wordnet')

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Veridion challenge/ml_insurance_challenge.csv')

def preprocess_text(text):
    if isinstance(text, float):
        text = str(text)
    stop_words = set(stopwords.words("english"))
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]
    return " ".join(tokens)

df['combined_text'] = df['business_tags'].apply(lambda x: ' '.join(eval(x))) + ' ' + df['category'] + df['niche']
df["processed_description"] = df["combined_text"].apply(preprocess_text)

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X = tfidf_vectorizer.fit_transform(df["processed_description"])

from sklearn.cluster import KMeans

num_clusters = 41
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
df["cluster"] = kmeans.fit_predict(X)


for cluster_id in range(num_clusters):
    cluster_data = df[df['cluster'] == cluster_id]
    print(f"Cluster {cluster_id}:")
    print(cluster_data[['category']])
    print("\n")

from sklearn.metrics import silhouette_score, davies_bouldin_score

silhouette_avg = silhouette_score(X, df['cluster'])
X_dense = X.toarray()

davies_bouldin_avg = davies_bouldin_score(X_dense, df['cluster'])

print(f"Silhouette Score: {silhouette_avg:.2f}")
print(f"Davies-Bouldin Score: {davies_bouldin_avg:.2f}")


model = SentenceTransformer('all-MiniLM-L6-v2')

cluster_29_data = df[df['cluster'] == 29]

remaining_data = df[df['cluster'] != 29]

remaining_embeddings = model.encode(remaining_data["processed_description"].tolist())
cluster_29_embeddings = model.encode(cluster_29_data["processed_description"].tolist())

scaler = StandardScaler()
remaining_embeddings_scaled = scaler.fit_transform(remaining_embeddings)
cluster_29_embeddings_scaled = scaler.transform(cluster_29_embeddings)

kmeans = KMeans(n_clusters=remaining_data['cluster'].nunique(), random_state=42)
kmeans.fit(remaining_embeddings_scaled)

predicted_clusters = kmeans.predict(cluster_29_embeddings_scaled)

df.loc[cluster_29_data.index, 'cluster'] = predicted_clusters

model = SentenceTransformer('all-MiniLM-L6-v2')

cluster_0_data = df[df['cluster'] == 0]

remaining_data = df[df['cluster'] != 0]


remaining_embeddings = model.encode(remaining_data["processed_description"].tolist())
cluster_0_embeddings = model.encode(cluster_0_data["processed_description"].tolist())


scaler = StandardScaler()
remaining_embeddings_scaled = scaler.fit_transform(remaining_embeddings)
cluster_0_embeddings_scaled = scaler.transform(cluster_0_embeddings)

kmeans = KMeans(n_clusters=remaining_data['cluster'].nunique(), random_state=42)
kmeans.fit(remaining_embeddings_scaled)

predicted_clusters = kmeans.predict(cluster_0_embeddings_scaled)

df.loc[cluster_0_data.index, 'cluster'] = predicted_clusters

insurance_category = {
     3: "Cleaning Operations",
     2: "Excavation",
     21: "Training Services",
     14: "Construction Services",
     37: "Construction Services",
     10: "Gas Installation",
     19: "Gas Installation",
     29: "Gas Installation",
     22: "Furniture Manufacturing",
     26: "Consulting Services",
     27: "Veterinary Services",
     8: "Property Management Services",
     17: "Volunteering Services",
     0: "Consulting Services",
     6: "Data Analysis Services",
     7: "Data Analysis Services",
     16: "Data Analysis Services",
     32: "Data Analysis Services",
     31: "Media Production Services",
     36: "Media Production Services",
     9: "Agricultural Services",
     38: "Agricultural Services",
     30: "Marketing Services",
     18: "Apparel Manufacturing",
     20: "Apparel Manufacturing",
     39: "Furniture Manufacturing",
     40: "Apparel Manufacturing",
     4: "Food Processing",
     15: "Food Processing",
     28: "Food Processing",
     11: "Fire Protection",
     23: "Paper Services",
     24: "Paper Services",
     35: "Paper Services",
     1: "Electrical Services",
     13: "Electrical Services",
     12: "Health Services",
     33: "Health Services",
     34: "Health Services",
     5: "Software Development",
     25: "House Services"
}

df['insurance_category'] = df['cluster'].map(insurance_category)

output_file_path = '/content/drive/MyDrive/Colab Notebooks/Veridion challenge/companies_with_insurance_categories.csv'
df.to_csv(output_file_path, index=False)
print(f"File saved to {output_file_path}")

